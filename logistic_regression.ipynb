{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:58:30.215075Z",
     "start_time": "2025-10-01T18:58:29.376169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import colormaps\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec2e2d353b27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"wildfires_training.csv\"\n",
    "test_file = \"wildfires_test.csv\"\n",
    "\n",
    "df = pd.read_csv(\"wildfires_training.csv\")\n",
    "df_test = pd.read_csv(\"wildfires_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f1ba7fb44b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfc9bfe9cf830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dab66af4f514c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ebddfe2498523",
   "metadata": {},
   "source": "## Pre-Processing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cc22f052dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction Months\n",
    "df[\"temp_humidity\"] = df[\"temp\"] * df[\"humidity\"]\n",
    "df[\"wind_bui\"] = df[\"wind_speed\"] * df[\"buildup_index\"]\n",
    "\n",
    "df_test[\"temp_humidity\"] = df_test[\"temp\"] * df_test[\"humidity\"]\n",
    "df_test[\"wind_bui\"] = df_test[\"wind_speed\"] * df_test[\"buildup_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a4056bf07d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = \"fire\"\n",
    "independent_variables = [col for col in df.columns if col != dependent_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba08fbd300790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training predictors and response variables\n",
    "X_training = df[independent_variables]\n",
    "y_training = df[dependent_variable] == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291be89bf9f15018",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[independent_variables]\n",
    "y_test = df_test[dependent_variable] == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0512ae9cc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to re-run with Hyper-parameter tuning\n",
    "penalties = [\"l1\", \"l2\", \"elasticnet\", None]\n",
    "c_values = np.logspace(-3, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90a2ba2a9c2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test_comparison = {}\n",
    "\n",
    "for penalty in penalties:\n",
    "    models_info = []\n",
    "    for c in c_values:\n",
    "        l1_ratio = 0.3 if penalty == \"elasticnet\" else None\n",
    "\n",
    "        if penalty == \"l1\":\n",
    "            solver = \"liblinear\"\n",
    "        elif penalty == \"elasticnet\":\n",
    "            solver = \"saga\"\n",
    "        else:\n",
    "            solver = \"lbfgs\"\n",
    "\n",
    "        logreg_pipe = Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\n",
    "                    \"logreg\",\n",
    "                    LogisticRegression(\n",
    "                        random_state=42,\n",
    "                        penalty=penalty,\n",
    "                        solver=solver,\n",
    "                        max_iter=5000,\n",
    "                        C=c if penalty is not None else 1.0,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        logreg_pipe.fit(X_training, y_training)\n",
    "        predictions_training = logreg_pipe.predict(X_training)\n",
    "        predictions_test = logreg_pipe.predict(X_test)\n",
    "\n",
    "        accuracy_training = metrics.accuracy_score(y_training, predictions_training)\n",
    "        accuracy_test = metrics.accuracy_score(y_test, predictions_test)\n",
    "\n",
    "        y_probs = logreg_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        models_info.append(\n",
    "            {\n",
    "                \"C\": c,\n",
    "                \"training_accuracy\": accuracy_training,\n",
    "                \"test_accuracy\": accuracy_test,\n",
    "                \"pipeline\": logreg_pipe,\n",
    "                \"predictions_test\": predictions_test,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Save all models for this penalty\n",
    "    training_test_comparison[penalty] = models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10203cf29ccec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = {}\n",
    "for pen in penalties:\n",
    "    best_model[pen] = max(\n",
    "        training_test_comparison[pen], key=lambda x: x[\"test_accuracy\"]\n",
    "    )\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81de154920defbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print nicely\n",
    "for penalty, model in best_model.items():\n",
    "    print(f\"Best {penalty if penalty else '(Without Regularization)'} model:\")\n",
    "    print(f\"  C = {model['C']}\")\n",
    "    print(f\"  Training Accuracy = {model['training_accuracy']:.4f}\")\n",
    "    print(f\"  Test Accuracy = {model['test_accuracy']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bf97e82c6277b",
   "metadata": {},
   "source": [
    "## Comparison Graphs\n",
    "Below are graphs showing the effect of penalty functions and C Values (Lambda Inverse) on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a07cc5aea7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(6, 6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, (penalty, model_info) in enumerate(best_model.items()):\n",
    "    y_pred = model_info[\"predictions_test\"]\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No\", \"Yes\"])\n",
    "    disp.plot(cmap=random.choice(list(colormaps)), ax=ax[i], colorbar=False)\n",
    "    disp.ax_.set_title(f\"Penalty: {penalty}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096cc889989fe75",
   "metadata": {},
   "source": "### Training and Test Set Accuracy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8a0b13fb80a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = training_test_comparison[\"l1\"]\n",
    "accuracy_training_l1 = [el[\"training_accuracy\"] for el in l1]\n",
    "accuracy_test_l1 = [el[\"test_accuracy\"] for el in l1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21e4f5eabd7c15",
   "metadata": {},
   "source": "#### L1 Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876b2d6f80998b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(c_values, accuracy_training_l1, marker=\"x\")\n",
    "plt.scatter(c_values, accuracy_test_l1, marker=\"+\")\n",
    "plt.xscale(\"log\")  # use log scale for C\n",
    "plt.ylim([0.0, 1.1])\n",
    "plt.xlabel(\"Value of C (log scale)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Training\", \"Test\"], loc=4)\n",
    "plt.title(\n",
    "    \"Effect of C on training and Test set accuracy on L1 Regularization\", fontsize=10\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b31c1a3535ba0d",
   "metadata": {},
   "source": "#### L2 Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee470566cc3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = training_test_comparison[\"l2\"]\n",
    "accuracy_training_l2 = [el[\"training_accuracy\"] for el in l2]\n",
    "accuracy_test_l2 = [el[\"test_accuracy\"] for el in l2]\n",
    "\n",
    "plt.scatter(c_values, accuracy_training_l2, marker=\"x\")\n",
    "plt.scatter(c_values, accuracy_test_l2, marker=\"+\")\n",
    "plt.xscale(\"log\")  # use log scale for C\n",
    "plt.ylim([0.0, 1.1])\n",
    "plt.xlabel(\"Value of C (log scale)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Training\", \"Test\"], loc=4)\n",
    "plt.title(\n",
    "    \"Effect of C on training and Test set accuracy on L2 Regularization\", fontsize=10\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7299cc864d9200",
   "metadata": {},
   "source": "#### ElasticNet Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e5ce9a31e9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet = training_test_comparison[\"elasticnet\"]\n",
    "accuracy_training_elasticnet = [el[\"training_accuracy\"] for el in elasticnet]\n",
    "accuracy_test_elasticnet = [el[\"test_accuracy\"] for el in elasticnet]\n",
    "\n",
    "plt.scatter(c_values, accuracy_training_elasticnet, marker=\"x\")\n",
    "plt.scatter(c_values, accuracy_test_elasticnet, marker=\"+\")\n",
    "plt.xscale(\"log\")  # use log scale for C\n",
    "plt.ylim([0.0, 1.1])\n",
    "plt.xlabel(\"Value of C (log scale)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Training\", \"Test\"], loc=4)\n",
    "plt.title(\"Effect of C on training and Test set accuracy on ElasticNet\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fd7ff821a6c85",
   "metadata": {},
   "source": "### Coefficient Magnitudes Vs C"
  },
  {
   "cell_type": "markdown",
   "id": "c54539bfccd9724a",
   "metadata": {},
   "source": "Elasticnet Coefficient Magnitude"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfd7c8177a4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_norms_elasticnet = [np.sum(np.abs(el[\"coefficients\"])) for el in elasticnet]\n",
    "plt.plot(c_values, coef_norms_elasticnet, marker=\"x\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"C (log scale)\")\n",
    "plt.ylabel(\"Sum of |coefficients|\")\n",
    "plt.title(\"Elasticnet Coefficient Magnitude vs C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75988cfec63502d",
   "metadata": {},
   "source": "#### L1 Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952113ff7c3a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_norms_l1 = [np.sum(np.abs(el[\"coefficients\"])) for el in l1]\n",
    "plt.plot(c_values, coef_norms_l1, marker=\"x\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"C (log scale)\")\n",
    "plt.ylabel(\"Sum of |coefficients|\")\n",
    "plt.title(\"L1 Coefficient Magnitude vs C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c80d659fcf5c90",
   "metadata": {},
   "source": "#### L2 Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18223b46729d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_norms_l2 = [np.sum(np.abs(el[\"coefficients\"])) for el in l2]\n",
    "plt.plot(c_values, coef_norms_l2, marker=\"x\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"C (log scale)\")\n",
    "plt.ylabel(\"Sum of |coefficients|\")\n",
    "plt.title(\"L1 Coefficient Magnitude vs C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aabc2687513011",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nonzero_elasticnet = [np.count_nonzero(el[\"coefficients\"]) for el in elasticnet]\n",
    "plt.plot(c_values, num_nonzero_elasticnet, marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"C (log scale)\")\n",
    "plt.ylabel(\"Number of Nonzero Coefficients\")\n",
    "plt.title(\"L1 Sparsity vs C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c836fefc32e3035",
   "metadata": {},
   "source": "### Coefficient Sparsity"
  },
  {
   "cell_type": "markdown",
   "id": "6c48e8e9bcec778e",
   "metadata": {},
   "source": "#### L2 Logistic Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf0c5313df860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nonzero_elasticnet = [np.count_nonzero(el[\"coefficients\"]) for el in elasticnet]\n",
    "plt.plot(c_values, num_nonzero_elasticnet, marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"C (log scale)\")\n",
    "plt.ylabel(\"Number of Nonzero Coefficients\")\n",
    "plt.title(\"ElasticNet Sparsity vs C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc662af0d60f5da1",
   "metadata": {},
   "source": "### ROC Curve"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5dc19bb6d09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pen in enumerate([\"l1\", \"l2\", \"elasticnet\", None]):\n",
    "    best_model = max(training_test_comparison[pen], key=lambda x: x[\"test_accuracy\"])\n",
    "    y_probs = best_model[\"pipeline\"].predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    plt.plot(fpr, tpr, label=f\"{pen} - AUC {auc(fpr, tpr)}\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d285c34b3fa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pen in enumerate([\"l1\", \"l2\", \"elasticnet\", None]):\n",
    "    best_model = max(training_test_comparison[pen], key=lambda x: x[\"test_accuracy\"])\n",
    "    title = f\"\\n\\t\\t\\t{pen.upper() if type(pen) == str else 'No Regularization'} Classification\"\n",
    "    print(f\"{title}\\n{classification_report(y_test, best_model['predictions_test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b0725a13d817f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
